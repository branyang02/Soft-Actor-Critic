{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6740c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import datetime\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import itertools\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.distributions import Normal\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a27a0f",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f49940e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_update(target, source, tau):\n",
    "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "        target_param.data.copy_(target_param.data * (1.0 - tau) + param.data * tau)\n",
    "\n",
    "def hard_update(target, source):\n",
    "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "        target_param.data.copy_(param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d54a059",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08af71ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_SIG_MAX = 2\n",
    "LOG_SIG_MIN = -20\n",
    "epsilon = 1e-6\n",
    "\n",
    "def weights_init_(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight, gain=1)\n",
    "        torch.nn.init.constant_(m.bias, 0)\n",
    "        \n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, hidden_dim):\n",
    "        super(QNetwork, self).__init__()\n",
    "\n",
    "        # Q1 architecture\n",
    "        self.linear1 = nn.Linear(num_inputs + num_actions, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.linear3 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "        # Q2 architecture\n",
    "        self.linear4 = nn.Linear(num_inputs + num_actions, hidden_dim)\n",
    "        self.linear5 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.linear6 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "        self.apply(weights_init_)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        xu = torch.cat([state, action], 1)\n",
    "        \n",
    "        x1 = F.relu(self.linear1(xu))\n",
    "        x1 = F.relu(self.linear2(x1))\n",
    "        x1 = self.linear3(x1)\n",
    "\n",
    "        x2 = F.relu(self.linear4(xu))\n",
    "        x2 = F.relu(self.linear5(x2))\n",
    "        x2 = self.linear6(x2)\n",
    "\n",
    "        return x1, x2\n",
    "\n",
    "\n",
    "class GaussianPolicy(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, hidden_dim, action_space=None):\n",
    "        super(GaussianPolicy, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(num_inputs, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.mean_linear = nn.Linear(hidden_dim, num_actions)\n",
    "        self.log_std_linear = nn.Linear(hidden_dim, num_actions)\n",
    "\n",
    "        self.apply(weights_init_)\n",
    "\n",
    "        # action rescaling\n",
    "        if action_space is None:\n",
    "            self.action_scale = torch.tensor(1.)\n",
    "            self.action_bias = torch.tensor(0.)\n",
    "        else:\n",
    "            self.action_scale = torch.FloatTensor(\n",
    "                (action_space.high - action_space.low) / 2.)\n",
    "            self.action_bias = torch.FloatTensor(\n",
    "                (action_space.high + action_space.low) / 2.)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.linear1(state))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        mean = self.mean_linear(x)\n",
    "        log_std = self.log_std_linear(x)\n",
    "        log_std = torch.clamp(log_std, min=LOG_SIG_MIN, max=LOG_SIG_MAX)\n",
    "        return mean, log_std\n",
    "\n",
    "    def sample(self, state):\n",
    "        mean, log_std = self.forward(state)\n",
    "        std = log_std.exp()\n",
    "        normal = Normal(mean, std)\n",
    "        x_t = normal.rsample()  # for reparameterization trick (mean + std * N(0,1))\n",
    "        y_t = torch.tanh(x_t)\n",
    "        action = y_t * self.action_scale + self.action_bias\n",
    "        log_prob = normal.log_prob(x_t)\n",
    "        # Enforcing Action Bound\n",
    "        log_prob -= torch.log(self.action_scale * (1 - y_t.pow(2)) + epsilon)\n",
    "        log_prob = log_prob.sum(1, keepdim=True)\n",
    "        mean = torch.tanh(mean) * self.action_scale + self.action_bias\n",
    "        return action, log_prob, mean\n",
    "\n",
    "    def to(self, device):\n",
    "        self.action_scale = self.action_scale.to(device)\n",
    "        self.action_bias = self.action_bias.to(device)\n",
    "        return super(GaussianPolicy, self).to(device)\n",
    "\n",
    "\n",
    "class DeterministicPolicy(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, hidden_dim, action_space=None):\n",
    "        super(DeterministicPolicy, self).__init__()\n",
    "        self.linear1 = nn.Linear(num_inputs, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.mean = nn.Linear(hidden_dim, num_actions)\n",
    "        self.noise = torch.Tensor(num_actions)\n",
    "\n",
    "        self.apply(weights_init_)\n",
    "\n",
    "        # action rescaling\n",
    "        if action_space is None:\n",
    "            self.action_scale = 1.\n",
    "            self.action_bias = 0.\n",
    "        else:\n",
    "            self.action_scale = torch.FloatTensor(\n",
    "                (action_space.high - action_space.low) / 2.)\n",
    "            self.action_bias = torch.FloatTensor(\n",
    "                (action_space.high + action_space.low) / 2.)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.linear1(state))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        mean = torch.tanh(self.mean(x)) * self.action_scale + self.action_bias\n",
    "        return mean\n",
    "\n",
    "    def sample(self, state):\n",
    "        mean = self.forward(state)\n",
    "        noise = self.noise.normal_(0., std=0.1)\n",
    "        noise = noise.clamp(-0.25, 0.25)\n",
    "        action = mean + noise\n",
    "        return action, torch.tensor(0.), mean\n",
    "\n",
    "    def to(self, device):\n",
    "        self.action_scale = self.action_scale.to(device)\n",
    "        self.action_bias = self.action_bias.to(device)\n",
    "        self.noise = self.noise.to(device)\n",
    "        return super(DeterministicPolicy, self).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff74ec3",
   "metadata": {},
   "source": [
    "## SAC Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b39cd63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAC(object):\n",
    "    def __init__(self, num_inputs, action_space):\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.policy_type = policy\n",
    "        self.target_update_interval = target_update_interval\n",
    "        self.automatic_entropy_tuning = automatic_entropy_tuning\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "\n",
    "        self.critic = QNetwork(num_inputs, action_space.shape[0], hidden_size).to(device=self.device)\n",
    "        self.critic_optim = Adam(self.critic.parameters(), lr=lr)\n",
    "\n",
    "        self.critic_target = QNetwork(num_inputs, action_space.shape[0], hidden_size).to(self.device)\n",
    "        hard_update(self.critic_target, self.critic)\n",
    "\n",
    "        if self.policy_type == \"Gaussian\":\n",
    "            # Target Entropy = −dim(A) (e.g. , -6 for HalfCheetah-v2) as given in the paper\n",
    "            if self.automatic_entropy_tuning is True:\n",
    "                self.target_entropy = -torch.prod(torch.Tensor(action_space.shape).to(self.device)).item()\n",
    "                self.log_alpha = torch.zeros(1, requires_grad=True, device=self.device)\n",
    "                self.alpha_optim = Adam([self.log_alpha], lr=lr)\n",
    "\n",
    "            self.policy = GaussianPolicy(num_inputs, action_space.shape[0], hidden_size, action_space).to(self.device)\n",
    "            self.policy_optim = Adam(self.policy.parameters(), lr=lr)\n",
    "\n",
    "        else:\n",
    "            self.alpha = 0\n",
    "            self.automatic_entropy_tuning = False\n",
    "            self.policy = DeterministicPolicy(num_inputs, action_space.shape[0], hidden_size, action_space).to(self.device)\n",
    "            self.policy_optim = Adam(self.policy.parameters(), lr=lr)\n",
    "\n",
    "    def select_action(self, state, evaluate=False):\n",
    "        state = torch.FloatTensor(state).to(self.device).unsqueeze(0)\n",
    "        if evaluate is False:\n",
    "            action, _, _ = self.policy.sample(state)\n",
    "        else:\n",
    "            _, _, action = self.policy.sample(state)\n",
    "        return action.detach().cpu().numpy()[0]\n",
    "\n",
    "    def update_parameters(self, memory, batch_size, updates):\n",
    "        # Sample a batch from memory\n",
    "        state_batch, action_batch, reward_batch, next_state_batch, mask_batch = memory.sample(batch_size=batch_size)\n",
    "\n",
    "        state_batch = torch.FloatTensor(state_batch).to(self.device)\n",
    "        next_state_batch = torch.FloatTensor(next_state_batch).to(self.device)\n",
    "        action_batch = torch.FloatTensor(action_batch).to(self.device)\n",
    "        reward_batch = torch.FloatTensor(reward_batch).to(self.device).unsqueeze(1)\n",
    "        mask_batch = torch.FloatTensor(mask_batch).to(self.device).unsqueeze(1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_state_action, next_state_log_pi, _ = self.policy.sample(next_state_batch)\n",
    "            qf1_next_target, qf2_next_target = self.critic_target(next_state_batch, next_state_action)\n",
    "            min_qf_next_target = torch.min(qf1_next_target, qf2_next_target) - self.alpha * next_state_log_pi\n",
    "            next_q_value = reward_batch + mask_batch * self.gamma * (min_qf_next_target)\n",
    "        qf1, qf2 = self.critic(state_batch, action_batch)  # Two Q-functions to mitigate positive bias in the policy improvement step\n",
    "        qf1_loss = F.mse_loss(qf1, next_q_value)  # JQ = 𝔼(st,at)~D[0.5(Q1(st,at) - r(st,at) - γ(𝔼st+1~p[V(st+1)]))^2]\n",
    "        qf2_loss = F.mse_loss(qf2, next_q_value)  # JQ = 𝔼(st,at)~D[0.5(Q1(st,at) - r(st,at) - γ(𝔼st+1~p[V(st+1)]))^2]\n",
    "        qf_loss = qf1_loss + qf2_loss\n",
    "\n",
    "        self.critic_optim.zero_grad()\n",
    "        qf_loss.backward()\n",
    "        self.critic_optim.step()\n",
    "\n",
    "        pi, log_pi, _ = self.policy.sample(state_batch)\n",
    "\n",
    "        qf1_pi, qf2_pi = self.critic(state_batch, pi)\n",
    "        min_qf_pi = torch.min(qf1_pi, qf2_pi)\n",
    "\n",
    "        policy_loss = ((self.alpha * log_pi) - min_qf_pi).mean() # Jπ = 𝔼st∼D,εt∼N[α * logπ(f(εt;st)|st) − Q(st,f(εt;st))]\n",
    "\n",
    "        self.policy_optim.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.policy_optim.step()\n",
    "\n",
    "        if self.automatic_entropy_tuning:\n",
    "            alpha_loss = -(self.log_alpha * (log_pi + self.target_entropy).detach()).mean()\n",
    "\n",
    "            self.alpha_optim.zero_grad()\n",
    "            alpha_loss.backward()\n",
    "            self.alpha_optim.step()\n",
    "\n",
    "            self.alpha = self.log_alpha.exp()\n",
    "            alpha_tlogs = self.alpha.clone() # For TensorboardX logs\n",
    "        else:\n",
    "            alpha_loss = torch.tensor(0.).to(self.device)\n",
    "            alpha_tlogs = torch.tensor(self.alpha) # For TensorboardX logs\n",
    "\n",
    "\n",
    "        if updates % self.target_update_interval == 0:\n",
    "            soft_update(self.critic_target, self.critic, self.tau)\n",
    "\n",
    "        return qf1_loss.item(), qf2_loss.item(), policy_loss.item(), alpha_loss.item(), alpha_tlogs.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b989e20",
   "metadata": {},
   "source": [
    "## Replay Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2ddf316",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    def __init__(self, capacity, seed):\n",
    "        random.seed(seed)\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = map(np.stack, zip(*batch))\n",
    "        return state, action, reward, next_state, done\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def save_buffer(self, env_name, suffix=\"\", save_path=None):\n",
    "        if not os.path.exists('checkpoints/'):\n",
    "            os.makedirs('checkpoints/')\n",
    "\n",
    "        if save_path is None:\n",
    "            save_path = \"checkpoints/sac_buffer_{}_{}\".format(env_name, suffix)\n",
    "        print('Saving buffer to {}'.format(save_path))\n",
    "\n",
    "        with open(save_path, 'wb') as f:\n",
    "            pickle.dump(self.buffer, f)\n",
    "\n",
    "    def load_buffer(self, save_path):\n",
    "        print('Loading buffer from {}'.format(save_path))\n",
    "\n",
    "        with open(save_path, \"rb\") as f:\n",
    "            self.buffer = pickle.load(f)\n",
    "            self.position = len(self.buffer) % self.capacity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1453e48",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a694643",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"HalfCheetah-v2\"\n",
    "policy = \"Gaussian\" # Gaussian or Deterministic\n",
    "eval = True\n",
    "gamma = 0.99\n",
    "tau = 0.005\n",
    "lr = 0.0003\n",
    "alpha = 0.2\n",
    "automatic_entropy_tuning = True\n",
    "seed = 123456\n",
    "batch_size = 256\n",
    "num_steps = 200_000\n",
    "hidden_size = 256\n",
    "updates_per_step = 1\n",
    "start_steps = 10_000\n",
    "target_update_interval = 1\n",
    "replay_size = 1_000_000\n",
    "cuda = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ec9060",
   "metadata": {},
   "source": [
    "## Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbd9142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1, total numsteps: 1000, episode steps: 1000, reward: -282.78\n",
      "Episode: 2, total numsteps: 2000, episode steps: 1000, reward: -364.1\n",
      "Episode: 3, total numsteps: 3000, episode steps: 1000, reward: -324.9\n",
      "Episode: 4, total numsteps: 4000, episode steps: 1000, reward: -314.22\n",
      "Episode: 5, total numsteps: 5000, episode steps: 1000, reward: -207.07\n",
      "Episode: 6, total numsteps: 6000, episode steps: 1000, reward: -383.9\n",
      "Episode: 7, total numsteps: 7000, episode steps: 1000, reward: -201.58\n",
      "Episode: 8, total numsteps: 8000, episode steps: 1000, reward: -231.79\n",
      "Episode: 9, total numsteps: 9000, episode steps: 1000, reward: -240.29\n",
      "Episode: 10, total numsteps: 10000, episode steps: 1000, reward: -363.41\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: -364.63\n",
      "----------------------------------------\n",
      "Episode: 11, total numsteps: 11000, episode steps: 1000, reward: -307.8\n",
      "Episode: 12, total numsteps: 12000, episode steps: 1000, reward: -222.99\n",
      "Episode: 13, total numsteps: 13000, episode steps: 1000, reward: -296.32\n",
      "Episode: 14, total numsteps: 14000, episode steps: 1000, reward: -232.73\n",
      "Episode: 15, total numsteps: 15000, episode steps: 1000, reward: -267.62\n",
      "Episode: 16, total numsteps: 16000, episode steps: 1000, reward: -118.53\n",
      "Episode: 17, total numsteps: 17000, episode steps: 1000, reward: -177.36\n",
      "Episode: 18, total numsteps: 18000, episode steps: 1000, reward: -145.41\n",
      "Episode: 19, total numsteps: 19000, episode steps: 1000, reward: -82.06\n",
      "Episode: 20, total numsteps: 20000, episode steps: 1000, reward: -272.96\n",
      "----------------------------------------\n",
      "Test Episodes: 10, Avg. Reward: -210.38\n",
      "----------------------------------------\n",
      "Episode: 21, total numsteps: 21000, episode steps: 1000, reward: -170.8\n",
      "Episode: 22, total numsteps: 22000, episode steps: 1000, reward: 216.55\n",
      "Episode: 23, total numsteps: 23000, episode steps: 1000, reward: 492.41\n",
      "Episode: 24, total numsteps: 24000, episode steps: 1000, reward: 555.94\n",
      "Episode: 25, total numsteps: 25000, episode steps: 1000, reward: 143.54\n",
      "Episode: 26, total numsteps: 26000, episode steps: 1000, reward: 965.02\n",
      "Episode: 27, total numsteps: 27000, episode steps: 1000, reward: 1910.32\n",
      "Episode: 28, total numsteps: 28000, episode steps: 1000, reward: 1111.19\n",
      "Episode: 29, total numsteps: 29000, episode steps: 1000, reward: 647.03\n"
     ]
    }
   ],
   "source": [
    "# Environment\n",
    "# env = NormalizedActions(gym.make(env_name))\n",
    "env = gym.make(env_name)\n",
    "env.seed(seed)\n",
    "env.action_space.seed(seed)\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Agent\n",
    "agent = SAC(env.observation_space.shape[0], env.action_space)\n",
    "\n",
    "#Tesnorboard\n",
    "writer = SummaryWriter('runs/{}_SAC_{}_{}_{}'.format(datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"), env_name,\n",
    "                                                             policy, \"autotune\" if automatic_entropy_tuning else \"\"))\n",
    "\n",
    "# Memory\n",
    "memory = ReplayMemory(replay_size, seed)\n",
    "\n",
    "# Training Loop\n",
    "total_numsteps = 0\n",
    "updates = 0\n",
    "\n",
    "for i_episode in itertools.count(1):\n",
    "    episode_reward = 0\n",
    "    episode_steps = 0\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "\n",
    "    while not done:\n",
    "        if start_steps > total_numsteps:\n",
    "            action = env.action_space.sample()  # Sample random action\n",
    "        else:\n",
    "            action = agent.select_action(state)  # Sample action from policy\n",
    "\n",
    "        if len(memory) > batch_size:\n",
    "            # Number of updates per step in environment\n",
    "            for i in range(updates_per_step):\n",
    "                # Update parameters of all the networks\n",
    "                critic_1_loss, critic_2_loss, policy_loss, ent_loss, alpha = agent.update_parameters(memory, batch_size, updates)\n",
    "\n",
    "                writer.add_scalar('loss/critic_1', critic_1_loss, updates)\n",
    "                writer.add_scalar('loss/critic_2', critic_2_loss, updates)\n",
    "                writer.add_scalar('loss/policy', policy_loss, updates)\n",
    "                writer.add_scalar('loss/entropy_loss', ent_loss, updates)\n",
    "                writer.add_scalar('entropy_temprature/alpha', alpha, updates)\n",
    "                updates += 1\n",
    "\n",
    "        next_state, reward, done, _ = env.step(action) # Step\n",
    "        episode_steps += 1\n",
    "        total_numsteps += 1\n",
    "        episode_reward += reward\n",
    "\n",
    "        # Ignore the \"done\" signal if it comes from hitting the time horizon.\n",
    "        # (https://github.com/openai/spinningup/blob/master/spinup/algos/sac/sac.py)\n",
    "        mask = 1 if episode_steps == env._max_episode_steps else float(not done)\n",
    "\n",
    "        memory.push(state, action, reward, next_state, mask) # Append transition to memory\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    if total_numsteps > num_steps:\n",
    "        break\n",
    "\n",
    "    writer.add_scalar('reward/train', episode_reward, i_episode)\n",
    "    print(\"Episode: {}, total numsteps: {}, episode steps: {}, reward: {}\".format(i_episode, total_numsteps, episode_steps, round(episode_reward, 2)))\n",
    "\n",
    "    if i_episode % 10 == 0 and eval is True:\n",
    "        avg_reward = 0.\n",
    "        episodes = 10\n",
    "        for _  in range(episodes):\n",
    "            state = env.reset()\n",
    "            episode_reward = 0\n",
    "            done = False\n",
    "            while not done:\n",
    "                action = agent.select_action(state, evaluate=True)\n",
    "\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                episode_reward += reward\n",
    "\n",
    "\n",
    "                state = next_state\n",
    "            avg_reward += episode_reward\n",
    "        avg_reward /= episodes\n",
    "\n",
    "\n",
    "        writer.add_scalar('avg_reward/test', avg_reward, i_episode)\n",
    "\n",
    "        print(\"----------------------------------------\")\n",
    "        print(\"Test Episodes: {}, Avg. Reward: {}\".format(episodes, round(avg_reward, 2)))\n",
    "        print(\"----------------------------------------\")\n",
    "    \n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a093a5",
   "metadata": {},
   "source": [
    "## Render the trained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64d9de63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating window glfw\n"
     ]
    }
   ],
   "source": [
    "test_state = env.reset()\n",
    "for i in range(5000):\n",
    "    test_action = agent.select_action(test_state)\n",
    "    test_state, __, __, _ = env.step(test_action)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e903f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f7a8cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
